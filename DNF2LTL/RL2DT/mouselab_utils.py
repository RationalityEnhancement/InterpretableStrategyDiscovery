import os
import pickle
from random import shuffle, randint
from frozendict import frozendict
from RL2DT.hyperparams import NUM_ACTIONS, TERM
from RL2DT.PLP.DSL import *

def load_demos(env_name, num_demos, except_path):
    '''
    Load the file with the policy's demonstrations located in the
    PLP_data folder as saved by strategy_demonstrations.py or use
    a custom path.

    Parameters
    ----------
    env_name : str
        Name of the environemnt to specify the search in the PLP_data folder
    num_demos : str
        Number of demos to take from the demonstration file
    except_path : str
        What path to the demonstrations file to use if they are not located
        in the PLP_folder or are not named according to strategy_demonstrations
        output

    Returns
    -------
    demos : dict
        pos: [ (Trial, action) ]
            Demonstrated (state, action) pairs
        neg: [ (Trial, action) ]
            (state, action) pairs for encountered states and non-optimal actions
            according to the RL policy
        leng_pos: [ int ]
            Lengths of the consecutive demonstrations
        leng_neg: [ int ]
            Lengths of the negative examples for the consecutive demonstrations
    '''
    cwd = os.getcwd()
    if except_path != '':
        with open(except_path, 'rb') as handle:
            demos = pickle.load(handle)
    else:
        two = '_2.pkl' if env_name == 'standard' else '2.pkl'
        env_name = '' if env_name == 'standard' else '_' + env_name
        
        with open(cwd+'/PLP_data/copycat_64'+ env_name + '.pkl', 'rb') as handle:
            demos = pickle.load(handle)

        with open(cwd+'/PLP_data/copycat_64'+ env_name + two, 'rb') as handle:
            add_demos = pickle.load(handle)

        demos = join(demos, add_demos)
    
    demos = {'pos': demos['pos'][:sum(demos['leng_pos'][:num_demos])], 
             'neg': demos['neg'][:sum(demos['leng_neg'][:num_demos])],
             'leng_neg': demos['leng_neg'][:num_demos], 
             'leng_pos': demos['leng_pos'][:num_demos]}

    return demos

def join(demos1, demos2):
    """
    Add two demonstrations dictionaries generated by strategy_demonstrations file. 


    Parameters
    ----------
    demos1 : dict
        pos: [ (Any, Any) ]
            Demonstrated (state, action) pairs
        neg: [ (Any, Any) ]
            (state, action) pairs for encountered states and non-optimal actions 
            according to the RL policy
        leng_pos: [ int ]
            Lengths of the consecutive demonstrations
        leng_neg: [ int ]
            Lengths of the negative examples for the consecutive demonstrations
    demos2 : dict

    Returns
    -------
    new_demos : dict
    """
    new_demos = {}
    new_demos['pos'] = demos1['pos'] + demos2['pos']
    new_demos['neg'] = demos1['neg'] + demos2['neg']
    new_demos['leng_pos'] = demos1['leng_pos'] + demos2['leng_pos']
    new_demos['leng_neg'] = demos1['leng_neg'] + demos2['leng_neg']

    return new_demos

def join_distrs(distr1, distr2):
    """
    Add two state-action distribution dictionaries. 


    Parameters
    ----------
    distr1 : dict
        int : Any
        A dictionary keyed by numbers coresponding to demonstrations
    distr2 : dict

    Returns
    -------
    new_distr : dict
    """
    new_distr = distr1
    num = len(distr1.keys())
    for k in distr2.keys():
        new_distr[k+num] = distr2[k]

    return new_distr

def solve_mouselab(formula, env, to_what=None):
    """
    Use the program found by the PLP method to act once in a particular 
    instatiation of the Mouselab MDP.

    Parameters
    ----------
    formula : [ StateActionProgram ]
    env : Trial
    to_what : [ int ]
        To what state reset the environment

    Returns
    -------
    actions : ( int )
        A tuple of taken actions
    reward : int
    state_actions_distribution : dict
        frozendict([ int ] : [ int ]): [ int ]
            A set of admissable actions for observed states written as [node_id] 
            : [value]
    """
    env.reset_observations(to_what=to_what)

    state = env
    actions = [ac for ac in range(NUM_ACTIONS)]
    clicks = tuple()
    state_actions_distribution = {}


    while 1:

        if (len(clicks) > 0 and clicks[-1] == TERM) or \
            len(clicks) > NUM_ACTIONS:
                break
            
        all_bad = True
        admissable_actions = []
        shuffle(actions)

        for act in actions:
            if formula(state, act):
                all_bad = False
                admissable_actions.append(act)

        if all_bad: 
            admissable_actions = [0]
            
        choice = randint(0,len(admissable_actions)-1)
        act = admissable_actions[choice]
        clicks = clicks + (act,)
        observed = [n.label for n in state.observed_nodes if n.label != 0]
        observed_vals = [n.value for n in state.observed_nodes if n.label != 0]
        froze_state = frozendict({k: v for k,v in zip(observed, observed_vals)})
        froze_action = frozenset(admissable_actions)
        state_actions_distribution[froze_state] = froze_action
    
        if act not in [n.label for n in state.observed_nodes]: 
            state.node_map[act].observe()


    best_path = [node for node in env.node_map.values() 
                 if is_on_highest_expected_value_path(state, node.label)]
    depths    = {i: [] for i in state.level_map.keys()}

    for node in best_path:
        depths[node.depth].append(node)

    best_path = []
    for dep, nodes in depths.items():
        best_path.append(nodes[0].value)

    reward = sum(best_path) - len(clicks) + 1
    if len(set(clicks)) != len(clicks): 
        reward  = 1e-10 ## penalty for clicking observed nodes; 
                        ## the formula is no good if it does that

    return clicks, reward, state_actions_distribution

def compare(main_dict, compare_dict):
    """
    Generate statistics based on the differences between two distributions of
    actions on a set of common states.

    Parameters
    ----------
    main_dict : dict
        A dictionary for the PLP program found by the method from Silver et al.
        Key is a state, value is a list of admissable actions
    compare_dict : dict
        A dictionary for the expert strategy of the form as above

    Returns
    -------
    percentages : [ int ]
        The percentage of states for which the admissable actions of the PLP 
        are [broader_than, same_as, included_in] the admissable actions of the 
        expert strategy
    output_frame : pd.DataFrame
        elements : ( int )
            A tuple representing the state
 
        size : string 
            The relation of the set of admissable PLP actions to the expert's 
            set actions_main : [ int ]
            Actions allowed by the PLP
            All if size == 'SAME' or 'LESS', only the additional when 
            size == 'MORE'
        actions_compare : [ int ]
            Actions allowed by the expert strategy
            All if size == 'SAME' or 'MORE', only the additional when
            size == 'LESS'
        likelihood_formula : float
            The probability for choosing an admissable action by the PLP
        likelihood_expert : float
            The probability for choosing an admissable action by the expert 
            strategy
    """
    all_main_dict = {}
    all_compare_dict = {}
    for k in main_dict.keys():
        all_main_dict = {**all_main_dict, **main_dict[k]}
    for k in compare_dict.keys():
        all_compare_dict = {**all_compare_dict, **compare_dict[k]}

    compare_keys = all_compare_dict.keys()
    common_keys = [key for key in all_main_dict.keys() if key in compare_keys]
    output_dict = {}

    for key in common_keys:
        if all_main_dict[key] == all_compare_dict[key]:
            output_dict[key] = ['same', 
                                all_main_dict[key], 
                                all_compare_dict[key], 
                                1./len(all_main_dict[key]), 
                                1./len(all_main_dict[key])]
            
        elif len(all_main_dict[key]) > len(all_compare_dict[key]):
            output_dict[key] = ['broader',
                                [el for el in all_main_dict[key] 
                                 if el not in all_compare_dict[key]],
                                all_compare_dict[key], 
                                1./len(all_main_dict[key]), 
                                1./len(all_compare_dict[key])]
                                
        else:
            output_dict[key] = ['narrower', 
                                all_main_dict[key], 
                                [el for el in all_compare_dict[key] 
                                 if el not in all_main_dict[key]], 
                                1./len(all_main_dict[key]), 
                                1./len(all_compare_dict[key])]

    all_states = len(output_dict.values())
   
    values = {k: tuple(k.values()) for k in output_dict.keys()}
    size = {k: v[0] for k,v in output_dict.items()}
    actions_formula = {k: v[1] for k,v in output_dict.items()}
    actions_expert = {k: v[2] for k,v in output_dict.items()}
    likelihood_formula = {k: v[3] for k,v in output_dict.items()}
    likelihood_expert = {k: v[4] for k,v in output_dict.items()}
    output_frame = pd.DataFrame({'values': values,
                                 'size': size,
                                 'actions_formula': actions_formula,
                                 'actions_expert': actions_expert,
                                 'likelihood_formula': likelihood_formula,
                                 'likelihood_expert': likelihood_expert})

    same_num = output_frame[output_frame['size'] == 'same'].shape[0]
    print("SAME:\n")
    print(same_num)
    distribution_same = output_frame[output_frame['size'] == 'same']
    print(distribution_same.to_string())
    print("\n\n")
    
    more_num = output_frame[output_frame['size'] == 'broader'].shape[0]
    print("BROADER:\n")
    print(more_num)
    distribution_broader = output_frame[output_frame['size'] == 'broader']
    print(distribution_broader.to_string())
    print("\n\n")
    
    less_num = all_states - same_num - more_num
    print("NARROWER:\n")
    print(less_num)
    distribution_narrower = output_frame[output_frame['size'] == 'narrower']
    print(distribution_narrower.to_string())
    print("\n\n")

    percentages = [float(same_num) / all_states, 
                   float(more_num) / all_states, 
                   float(less_num) / all_states]
        
    return percentages, distribution_broader, distribution_narrower

def env_to_string(env):
    return env.ground_truth

def load_additional_data(parsed_args, num_demos, double_files=True):
    '''
    Load addiitonal statistics that enable us to compare the output
    of the interpret algorithm to the expert. It is Mouselab specific.

    Files are listed in the Returns section.
    
    Default location is PLP_data folder. Default names are the ones used by
    strategy_demonstrations.py. The only files needed for the algorithms to work
    are demos and expert_reward.

    Parameters
    ----------
    parsed_args : ArgumentParser.Namespace
    num_demos : int
    double_file : bool

    Returns
    -------
    valid_envs : [ Any ]
    valid_actions :  [ [ int ] ]
    valid_reward : [ int ]
    valid_distribution: dict
    
    See interpret_binary for the description of the variables
    '''
    args = parsed_args
    cwd = os.getcwd()
    
    env_name = '_' + args.environment if args.environment != 'standard' else ''
    END_PATH = str(BASE_NUM_DEMOS) + env_name + '.pkl'
    END_ACTIONS_PATH = str(BASE_NUM_DEMOS) + "_" + str(STATS_ROLLOUTS) + env_name + '.pkl'
    valid_env_path = cwd+'/PLP_data/test_envs_copycat_' + END_PATH
    valid_actions_path = cwd+'/PLP_data/test_actions_copycat_' + END_ACTIONS_PATH
    valid_distribution_path = cwd+'/PLP_data/distribution_copycat_' + END_PATH
           
    print('Loading the additional files...')

    with open(test_env_path, 'rb') as handle:
        envs = pickle.load(handle)

    with open(valid_actions_path, 'rb') as handle:
        actions, valid_reward = pickle.load(handle)

    with open(test_distribution_path, 'rb') as handle:
            distribution = pickle.load(handle)

    if double_files:
        two = '_2.pkl' if env_name == '' else '2.pkl'

        with open(re.sub('.pkl', two, valid_env_path), 'rb') as handle:
            add_envs = pickle.load(handle)

        with open(re.sub('.pkl', two, valid_actions_path), 'rb') as handle:
            add_actions, add_valid_reward = pickle.load(handle)

        with open(re.sub('.pkl', two, valid_distribution_path), 'rb') as handle:
            add_distribution = pickle.load(handle)


        envs += add_envs
        actions += add_actions
        valid_reward += add_valid_reward
        distribution = join_distrs(distribution, add_distribution)

    if num_demos == -1: num_demos = len(demos['pos'])

    valid_actions = actions
    valid_envs = envs[:num_demos]
    valid_reward = valid_reward[:num_demos]
    valid_distribution = {i: distribution[i] for i in range(num_demos)} if distribution \
                    else None

    return valid_envs, valid_actions, valid_reward, valid_distribution
